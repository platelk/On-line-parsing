After some research, we can see 2 important types of tool used inside IDE for programming language analysis :
\begin{itemize}
\item The analyser and the parser are based on the language tools (used inside the compiler or interpreter of the language) as C++ with clang analyser \cite{Clang5:online} or Dartlang with dartanalyser\cite{darta0:online} or Golang with golinter \cite{golan5:online}.
\item The parser have been "re-wrote" to feet in IDE requirement and can offer to some tools or plug-ins inside the IDE. The possibility to provide a grammar definition and some function will be "trigger" (called) by the IDE when the parser will find a element that match the provided grammar.
\end{itemize}
In the first case, the tools use a parser adapted to the requirement of the language (error recovering or speed ...) but will not necessarily consider the use case of someone continuously typing some code or doing a copy \/ paste of large block of code. This choices are perfectly adapted in the case of a compiler or a interpreter. In this case you can assume that the input code have to be correct and refuse a piece of code because it miss a semi-columns.
\\
In the second case, the parser will try as more as possible to provide an useful feedback to the developer. Some parsers are capable of predict what input is expected with a large capability of error-recovering. This 2 factors are great for the developer (here the user) but, in some case, this features can slow down the parser. When it happens on 2 lines it's not really important, but when you paste 50 lines of code in the middle of a big file it can slow down significantly the process (ex: The starting time of Eclipse, or when you paste a lot of line inside the IDE).
\\
\\
So each technique have its advantages and its drawbacks, but a composite of the two can be an interesting solution. This approach can solve the problem of being quick as possible and provide useful feedback to the developer.
\\
The idea is based on observation and research :
\begin{itemize}
\item Bottom-up algorithm are generally faster but less predictive
\item Top-down algorithm are generally more predictive but slower
\end{itemize}

So based on that, bottom-up algorithms can be more useful for big file or big edition inside a file, because it will treat all the information quicker and based on experience :
\begin{itemize}
\item When an important edition is made, it is often a paste from an other source where the syntax (not necessarily the variables name, ...) will be correct.
\item When you open a big file, in a lot of case a large part of the file will be correct, because it have been generated, or because the developer (user) work on a piece of this big file and not all of it.
\end{itemize}

Top-down algorithms are very useful for error-recovering and prediction and so can give useful feedback and information to the developer. They are very powerful when working on smaller input, because base on experience :
\begin{itemize}
\item A lot of error made when programming are careless mistake, that can easily be corrected without the whole context (the function context can be enough).
\item A prediction of what can be expected (or not) next, can be very useful.
\\ example :
\begin{lstlisting}[language=C++, caption=simple prediction example in C]
// cursor after the '9' can expect '+', '*', '/', '-', '\%', ';'
int a = 9 
\end{lstlisting}
Here, a simplification of the prediction can be displayed as "next: 'mathematical expression' or 'end statement' " because a top-down algorithm will know which "node" can be expected next.
\end{itemize}
Based on this information, one path to explore can be a combination of the 2 algorithm based on the context by using the same syntax tree and collect as much as possible information during the parsing process.