Many techniques have been used to speed up development process through time on different sector of the development process, on the compilation through incremental compilation and compilation optimisation, and cache file to save result and computation time or, one of the main part of this tools, parser.
Many parsing algorithm exist but haven't necessarily think for the context of a IDE (Integrate Development Environment).
\\We will see different parsing algorithm and their advantage and disadvantage.
\subsection{Parser}
Before continuing further, an important point is to define what is a parser. A parser is a program that will take a input (usually a string) and will transform it into a structure more understandable by a computer program (usually a graph or a tree).\\

\subsection{Parsing technique}
One of the main part of a linters (or syntax analyser) is the parser. it's the principal part that will define the behaviour of tools, because a top-down parser will not allow the same thing than a bottom up parser.

\subsubsection{Context and Grammar}
First of all, a grammar is a set of terminal rules with terminal symbol (like characters) and non-terminal rules which are a set of terminal and non-terminal rules.
An important thing to know is the context where the parser will operate, and one big point is to know the grammar that the parser have to work with. 4 type of grammar exist\cite{grune2008parsing} and each type allow a grammar to be more or less ambiguous, which means that it's more or less simple to chose between 2 grammar rules with the same parsed information, and this "level" of ambiguity will define the difficulty to write the parser and the capacity of deductions.
\\In fact Context Free (CF) grammar \cite{cfg_def}, a grammar where the production rules is a single non-terminal rules that produce one or multiple terminal or non-terminal rules, are easier to parse and to give better feedback in case of error because it's simpler to predict what are the next possible symbol in this context \cite{grune2008parsing}, actually most of programming language are based on CF grammar, that allow syntax analyser to offer what's are called "Auto-completion" to developers. 

\subsubsection{Grammar}
As said above, grammar are an important component of parser and give most of the constraint the parser will have to manage. In fact, parse a URL and parse a complete programming language is not the same thing. First, a grammar is a set of rules, where each rules are describe by a set of rules on the left of the assignment operator, that will be produced if the set of rules on the right are present in the same order.\\
For instance, a rule can be described like "B ::= A 'a' 'b';", here B can be produced if the rule A and the character 'a' and 'b' are parsed. Another interesting thing is the difference between terminal and non-terminal rules. Terminal rules are rules where all the rules on the right are describe as character, here 'a' and 'b' are terminal but the rule B is not.\\
To describe this difference of complexity, All language are describe by different type of grammar, for example, an URL can be described with a grammar of type 3 and a programming language like Javascript can be described by a grammar of type 2. Researchers actually agree that all the grammar can be classified in 4 types from a grammar with recursivity inside the definition and with a mathematical proof that it's impossible to create a parser generator for this kind of grammar to grammar that are only defined by one terminal rule.\\
Grammar can be represented by different forms, like BNF (Backus-Naur Form) or CNF (Chomsky normal from) that are different ways of write and describe a grammar, some forms can impose some constraints, as CNF impose to eliminate right-hand sides with more than 2 non-terminals, and some algorithm transform some form to others to simplify the generation of parser or parsing table.\\
Even if a lot of the mathematical research is done on grammar and grammar type, some algorithm are still created to transform grammar and make them simpler or easier to use.

\subsubsection{Grammar Table}
Grammar table is a tool used by a lot of popular parsers today like LR or LL parser. Grammar table are table that associate a state with the next element parsed and the action to do. With this kind of table each time you parse an element and the actual state in the parser, you can know which action you have to do and the next state of your parser.\\
This kind of table are very interesting for parsers because in many case this table is generated and so it will be the exact same table for the same grammar. This pre-computation will be computation saved during the parsing and so better performance can be achieved. An other interesting point is the optimisation done during the generation, most of the grammar table are generated by transforming a table to a NFSA (Non Finite State Automaton) then to a FSA (Finite State Automaton), during this procedure, a lot of optimisation can be done, like removing impossible production rules, transform non-terminal to terminal rules, ...\\
Grammar table are a open field of research, each parser having its own table and focus on different aspect as predictability or performance.

\subsubsection{Top down}
Top down parser will try, at each symbol read, to create the whole syntax tree. Syntax trees are logical tree structure where each node store the parsed symbol and the syntactic rules it correspond, possibly based on which next symbol the parser can expect \cite{knuth1971top}\cite{grune2008parsing}. 
\\One of the most known top-down parser family are LL parsers. this kind of parser which are based on prediction and expectation, are really good when a parsing error occurs (often because it can not, or don't know how to, build the next node) to go back to a previous stable state or other prediction branch. This is call "error recovering"\cite{panopticoncentral_2009}. This can be exactly what we want of a good syntax analyser, the trade off in computation time to create all this syntax tree, specially when the user will modify intensively the file.

\subsubsection{Bottom up}
Bottom up parser are the opposite way of thinking. On each symbol, the parser will try to create a terminal token (a word, a keyword) and when the parser have a terminal token, it will try to "shift" these token to create an "upper" token\cite{knuth1971top}\cite{grune2008parsing}.
\\One of the most known bottom-up parser family of LR parsers. This kind of parser are more sensible to ambiguity because if in the step of shifting, the accumulate token can be transform in 2 "upper" token, the parser will fail. To prevent this behaviour a sub family of LR parsers called LR(*) are used. LR(*) parser ('*' is the token) will read one token, and if an error happens, the parser will read  more tokens to determine which rule make more sense. But for each other token read the complexity and the computation time increase, this is why the most used is the LR(1).
\\Bottom up parser are quick, but compare to top-down parser, it can be more sensible to errors even with LR(1), but with the today's technology, specially the increasing number of parallel process our CPU can handle a various of the LR and LR(*) parsers called GLR for "Generalise LR", where when the parser find a ambiguous situation and thread will be created for each possible syntax tree\cite{panopticoncentral_2009} and will kill the thread if it's impossible to resolve this scenarios. This algorithm shows really great performance and error recovering, but can use more resources.

\subsubsection{Incremental parsing}
Incremental parsing refer to parser which will try to detect each change made and will modify the syntax tree without recreate the whole tree.\cite{grune2008parsing}\cite{horspool1990incremental}
\\Every parsing algorithm can be modified to be used as "incremental parser", but they are often combined with a prediction algorithm to try improve the error recovering, because when you can predict what can happen next, you can pre-process and so improve the speed.
\\A technique used is to have 1 syntax tree and 1 linear representation of the data, and on each modification on the linear representation, the parser will try to modify the tree with the minimum impact and computation.
